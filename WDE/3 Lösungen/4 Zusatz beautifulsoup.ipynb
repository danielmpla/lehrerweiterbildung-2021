{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gorgeous-charm",
   "metadata": {},
   "source": [
    "# Extraktion aller Newsartikel von der Website\n",
    "\n",
    "Erweitern Sie bitte das in der Demo vorgestellte Beispiel zum Extrahieren der News von der Hochschulseite, um das iterieren aller verfügbaren Newsseiten. Selektieren Sie hierfür mittels Beautiful Soup den \"weitere Einträge\"-Link und Laden die Folgeseite auf dieser Basis. Fahren Sie solange fohrt, wie dieser Button zur Verfügung steht (Hinweis: max. 10 Seiten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Methode zum extrahieren der News Informationen (aus Demo entnommen) \n",
    "def extract_article_information(article):\n",
    "    article_info = {}\n",
    "    article_info[\"heading\"] = article.span.string \n",
    "    article_info[\"teaser\"] = article.select_one(\".news-article__teaser\").text.strip()\n",
    "    article_info[\"date\"] = article.select_one(\".news-article__date\").text\n",
    "    article_info[\"link\"] = article.find(\"a\").attrs[\"href\"]\n",
    "    return article_info\n",
    "        \n",
    "# Trennung der url, da der Link des <a>-Tags von der base_url ausgeht  \n",
    "base_url = \"https://www.hszg.de\"\n",
    "news_url = base_url + \"/hochschule/aktuelles/neuigkeiten/page/1\"\n",
    "\n",
    "#Lädt die URL, extrahiert die News und liefert das BeautifulSoup-Objekt zum weiteren selektieren zurück.\n",
    "def extract_articles_from_page(url):\n",
    "\n",
    "    # Seite laden und parsen\n",
    "    request = requests.get(url)\n",
    "    html = request.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Artikel Container selektieren\n",
    "    news_articles = soup.find_all(class_=\"news-article\")\n",
    "    \n",
    "    # jeden Artikel im Container extrahieren und der globalen Liste hinzufügen\n",
    "    for article in news_articles:\n",
    "        articles.append(extract_article_information(article))\n",
    "    return soup\n",
    "        \n",
    "# Globale Liste instanziiren zum speichern aller Atrikel\n",
    "articles = []\n",
    "\n",
    "# erste Seite laden und parsen \n",
    "soup = extract_articles_from_page(news_url)\n",
    "\n",
    "# Direktes extrahieren der [\"href\"] infos führt zu einem Fehler wenn <a> Tag None ist.\n",
    "navigation_tag = soup.find(\"div\", class_=\"news__button-link button--right\").a\n",
    "\n",
    "# Deshalb prüfen wir hier, auf vorhandensein des Links und iterieren, solange dieser verfügbar ist.\n",
    "while navigation_tag is not None:\n",
    "\n",
    "    # neuen Abfragelink aus base_url und dem href des <a>-Tags erstellen\n",
    "    link = base_url + navigation_tag[\"href\"]\n",
    "    print(\"Extract content from: \" + link)\n",
    "    # neue Seite vom Server laden\n",
    "    \n",
    "    soup = extract_articles_from_page(link)\n",
    "    \n",
    "    # wichtig da als Abbruchbedingung genutzt\n",
    "    navigation_tag = soup.find(\"div\", class_=\"news__button-link button--right\").a\n",
    "    \n",
    "    # 3 Sekunden warten\n",
    "    time.sleep(3)\n",
    "\n",
    "print(articles)\n",
    "assert len(articles) == 100, 'Sie haben ' + str(len(articles)) + ' News geladen, aber 100 werden erwartet!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a795a-4629-4120-9aa3-1fba38bfe07e",
   "metadata": {},
   "source": [
    "# Lösungshinweise:\n",
    "\n",
    "Bei Webdatenextraktion führen wie im wahren Leben, sprichwörtlich viele Wege nach Rom.\n",
    "Folgende Möglichkeiten zum selektieren des Links wären bspw. denkbar gewesen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52850ab3-f777-4634-b74c-05c2c00c9c53",
   "metadata": {},
   "source": [
    "```python\n",
    "# Varianten zum extrahieren des Links\n",
    "# 1. div selektieren\n",
    "next_link_div = soup.find(\"div\", class_=\"news__button-link button--right\")\n",
    "next_link_div = soup.find_all(\"div\", class_=\"news__button-link button--right\")[0]\n",
    "next_link_div = soup.select_one(\"div.news__button-link.button--right\")\n",
    "next_link_div = soup.select(\"div.news__button-link.button--right\")[0]\n",
    "\n",
    "# 2. <a> separieren bzw Link auslesen\n",
    "next_link_a = next_link_div.a\n",
    "next_link_a = next_link_div.find(\"a\")\n",
    "next_link_a = next_link_div.find_all(\"a\")[0]\n",
    "next_link_a = next_link_div.select_one(\"a\")\n",
    "next_link_a = next_link_div.select(\"a\")[0]\n",
    "\n",
    "# 3. Link extrahieren\n",
    "next_link = next_link_a[\"href\"]\n",
    "\n",
    "# alternativ in einem Aufruf durch Kombination z.B.\n",
    "next_link = soup.find(\"div\", class_=\"news__button-link button--right\").a[\"href\"]\n",
    "next_link = soup.select_one(\"div.news__button-link.button--right > a\")[\"href\"]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
