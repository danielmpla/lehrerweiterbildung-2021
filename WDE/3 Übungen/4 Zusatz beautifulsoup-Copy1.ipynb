{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gorgeous-charm",
   "metadata": {},
   "source": [
    "# Extraktion aller Newsartikel von der Website\n",
    "\n",
    "Erweitern Sie bitte das in der Demo vorgestellte Beispiel zum Extrahieren der News von der Hochschulseite, um das iterieren aller verfügbaren Newsseiten. Selektieren Sie hierfür mittels Beautiful Soup den \"weitere Einträge\"-Link und Laden die Folgeseite auf dieser Basis. Fahren Sie solange fohrt, wie dieser Button zur Verfügung steht (Hinweis: max. 10 Seiten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Methode zum extrahieren der News Informationen (aus Demo entnommen) \n",
    "def extract_article_information(article):\n",
    "    article_info = {}\n",
    "    article_info[\"heading\"] = article.span.string \n",
    "    article_info[\"teaser\"] = article.select_one(\".news-article__teaser\").text.strip()\n",
    "    article_info[\"date\"] = article.select_one(\".news-article__date\").text\n",
    "    article_info[\"link\"] = article.find(\"a\").attrs[\"href\"]\n",
    "    return article_info\n",
    "        \n",
    "# Trennung der url, da der Link des <a>-Tags von der base_url ausgeht  \n",
    "base_url = \"https://www.hszg.de\"\n",
    "news_url = base_url + \"/hochschule/aktuelles/neuigkeiten/page/1\"\n",
    "\n",
    "#Lädt die URL, extrahiert die News und liefert das BeautifulSoup-Objekt zum weiteren selektieren zurück.\n",
    "def extract_articles_from_page(url):\n",
    "\n",
    "    # Seite laden und parsen\n",
    "    request = requests.get(url)\n",
    "    html = request.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Artikel Container selektieren\n",
    "    news_articles = soup.find_all(class_=\"news-article\")\n",
    "    \n",
    "    # Alle gefunden Artikel der globalen Artikle-Liste hinzufügen\n",
    "    for article in news_articles:\n",
    "        articles.append(extract_article_information(article))\n",
    "    return soup\n",
    "        \n",
    "# Globale Liste instanziiren zum speichern aller Atrikel\n",
    "articles = []\n",
    "\n",
    "# erste Seite laden und parsen \n",
    "soup = extract_articles_from_page(news_url)\n",
    "\n",
    "#Hier Ihr Code:\n",
    "#TODO 1) Selektieren sie den Link \"weitere Einträge\" ...\n",
    "#TODO 2) ... und laden auf dessen Basis die Folgeseiten (base_url + Link (href))\n",
    "#solange der Link \"weitere Einträge\" verfügbar ist\n",
    "#ACHTUNG: Bitte nutzen sie 'time.sleep(3)' zwischen einem jeden Ladevorgang,\n",
    "#um den Server nicht zu überlasten\n",
    "time.sleep(3)\n",
    "\n",
    "print(articles)\n",
    "assert len(articles) == 100, 'Sie haben ' + str(len(articles)) + ' News geladen, aber 100 werden erwartet!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
